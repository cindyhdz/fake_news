{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab8cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-win_amd64.whl (444.0 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vida_\\anaconda3\\envs\\mlenv\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=ffc09b89505ed1a4c7424df4f21de308f950fc6d018f5a540143ffb2301ecb55\n",
      "  Stored in directory: c:\\users\\vida_\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\vida_\\anaconda3\\envs\\mlenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\vida_\\anaconda3\\envs\\mlenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\vida_\\anaconda3\\envs\\mlenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d2c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Data\n",
    "fake_test_data = pd.read_csv('./Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dataset\n",
    "fake_news_df = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea8512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Merge data for new column\n",
    "\n",
    "fake_news_df[\"merged\"] = fake_news_df[\"title\"]+\" \"+fake_news_df[\"author\"]\n",
    "\n",
    "fake_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  check null values\n",
    "fake_news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "fake_news_df=fake_news_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f19532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now count the Unique values using np.unique to find the elements of an array as parameter\n",
    "count = np.unique(fake_news_df['label'], return_counts=True)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data = fake_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent Features\n",
    "Y = fake_news_df['label']\n",
    "\n",
    "# Independent Features\n",
    "X = fake_news_df.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9619495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dataset into numerical representation\n",
    "# make the copy of Dependent Featues and reset the index because we drop the nan values due to this \n",
    "# the index is disturb\n",
    "messages = X.copy()\n",
    "# messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96733dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342eb3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35161aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['merged'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4887778",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = 5000\n",
    "onehot_repre = [one_hot(words, voc_size) for words in corpus]\n",
    "onehot_repre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ecf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "sent_length = 20\n",
    "embedded_docs = pad_sequences(onehot_repre, padding='pre', maxlen=sent_length)\n",
    "print(embedded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_features = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final = np.array(embedded_docs)\n",
    "y_final = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final.shape, y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT THE DATA INTO TRAIN AND TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAINING\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE METRICS AND ACCURACY\n",
    "classes_x = (model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "score_0 = accuracy_score(y_test,classes_x)\n",
    "print(score_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70813006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24488\\102728674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpred_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# cr1    = classification_report(y_test,pred_1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(cr1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "model_1 = LogisticRegression(max_iter=900)\n",
    "model_1.fit_resample(X_train,y_train)\n",
    "pred_1 = model_1.predict(X_test)\n",
    "# cr1    = classification_report(y_test,pred_1)\n",
    "# print(cr1)\n",
    "score_1 = accuracy_score(y_test,pred_1)\n",
    "print(score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee5b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7265473",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomOverSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24488\\1684902982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#osx, osy = OS.fit_sample(X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mx_sam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_sam\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mpred_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomOverSampler' is not defined"
     ]
    }
   ],
   "source": [
    "# Resample the training data with the RandomOversampler\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#OS = RandomOverSampler(sampling_strategy='auto', random_state=0)\n",
    "#osx, osy = OS.fit_sample(X, y)\n",
    "\n",
    "model_3 = RandomOverSampler(random_state=1)\n",
    "x_sam,y_sam =model_3.fit_resample(X_train,y_train)\n",
    "pred_3 = model_3.predict(X_test)\n",
    "# cr3    = classification_report(y_test,pred_3)\n",
    "# print(cr3)\n",
    "score_3 = accuracy_score(y_test,pred_3)\n",
    "print(score_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f303076",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24488\\1085062416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpred_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# cr4    = classification_report(y_test,pred_4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(cr4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "model_4 = RandomForestClassifier()\n",
    "model_4.fit(X_train,y_train)\n",
    "pred_4 = model_4.predict(X_test)\n",
    "# cr4    = classification_report(y_test,pred_4)\n",
    "# print(cr4)\n",
    "score_4 = accuracy_score(y_test,pred_4)\n",
    "print(score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca2ecb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24488\\2796198408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClusterCentroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpred_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# cr5    = classification_report(y_test,pred_5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "model_5 = ClusterCentroids()\n",
    "model_5.fit(X_train,y_train)\n",
    "pred_5 = model_5.predict(X_test)\n",
    "# cr5    = classification_report(y_test,pred_5)\n",
    "# print(cr5)\n",
    "score_5 = accuracy_score(y_test,pred_5)\n",
    "print(score_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data with SMOTEENN\n",
    "# Warning: This is a large dataset, and this step may take some time to complete\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "model_6 = SMOTEENN()\n",
    "model_6.fit(X_train,y_train)\n",
    "pred_6 = model_6.predict(X_test)\n",
    "# cr6    = classification_report(y_test,pred_6)\n",
    "# print(cr6)\n",
    "score_6 = accuracy_score(y_test,pred_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdc74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91482b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
